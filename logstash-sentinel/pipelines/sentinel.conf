input {
  kinesis {
    kinesis_stream_name => "${kinesis_stream_name}"
    region => "${region}"
    application_name => "${application_name}"
    checkpoint_interval_seconds => 10
    codec => json
    metrics => "cloudwatch"
  }
}

filter {
    if [headers] {
       mutate {
          remove_field => ["headers", "host"]
       }
    }

    grok {
        # attempt to parse syslog lines
        match => { "message" => "%{SYSLOG5424PRI}%{NONNEGINT:syslog_ver} +(?:%{TIMESTAMP_ISO8601:syslog_timestamp}|-) +(?:%{HOSTNAME:syslog_host}|-) +(?:%{NOTSPACE:syslog_app}|-) +(?:%{NOTSPACE:syslog_proc}|-) +(?:%{WORD:syslog_msgid}|-) +(?:%{SYSLOG5424SD:syslog_sd}|-|) +%{GREEDYDATA:syslog_msg}" }
        # if successful, save original `@timestamp` and `host` fields created by logstash
        add_field => [ "received_at", "%{@timestamp}" ]
        tag_on_failure => ["_syslogparsefailure"]
    }

    # parse the syslog pri field into severity/facility
    syslog_pri { syslog_pri_field_name => 'syslog5424_pri' }

    # replace @timestamp field with the one from syslog
    date { match => [ "syslog_timestamp", "ISO8601" ] }

    if !("_syslogparsefailure" in [tags]) {
        # if we successfully parsed syslog, replace the message and source_host fields
        mutate {
            replace => [ "source_host", "%{syslog_host}" ]
            replace => [ "message", "%{syslog_msg}" ]
        }
    }

    # Cloud Foundry passes the app name, space and organisation in the syslog_host
    # Filtering them into separate fields makes it easier to query multiple apps in a single Kibana instance
    dissect {
        mapping => { "syslog_host" => "%{[cf][org]}.%{[cf][space]}.%{[cf][app]}" }
        tag_on_failure => ["_sysloghostdissectfailure"]
    }

    # Cloud Foundry gorouter logs
    if [syslog_proc] =~ "RTR" {
        grok {
            match => { "syslog_msg" => "%{HOSTNAME:[access][host]} - \[%{TIMESTAMP_ISO8601:router_timestamp}\] \"%{WORD:[access][method]} %{NOTSPACE:[access][url]} HTTP/%{NUMBER:[access][http_version]}\" %{NONNEGINT:[access][response_code]:int} %{NONNEGINT:[access][body_received][bytes]:int} %{NONNEGINT:[access][body_sent][bytes]:int} %{QUOTEDSTRING:[access][referrer]} %{QUOTEDSTRING:[access][agent]} \"%{HOSTPORT:[access][remote_ip_and_port]}\" \"%{HOSTPORT:[access][upstream_ip_and_port]}\" %{GREEDYDATA:router_keys}" }
            tag_on_failure => ["_routerparsefailure"]
            add_tag => ["gorouter"]
        }
        # replace @timestamp field with the one from router access log
        date {
            match => [ "router_timestamp", "ISO8601" ]
        }
        kv {
            source => "router_keys"
            target => "router"
            value_split => ":"
            remove_field => "router_keys"
        }
    }

    if [syslog_proc] =~ "APP" {
       mutate {
          add_tag => ["app"]
       }
    }

    # Application logs
    #if [syslog_proc] =~ "APP" and [syslog_msg] != "" and [syslog_msg] != "\n" {
    #    json {
    #        source => "syslog_msg"
    #        add_tag => ["json"]
    #        skip_on_invalid_json => true
    #    }
    #}

    # User agent parsing
    if [access][agent] {
        useragent {
            source => "[access][agent]"
            target => "[access][user_agent]"
        }
    }

    if [cf][app] != "staff-sso" and [cf][space] != "market-access" and [cf][app] != "datahub-api" {
      drop { }
    }
}

output {
  microsoft-logstash-output-azure-loganalytics {
    workspace_id => "${SENTINEL_WORKSPACE_ID}"
    workspace_key => "${SENTINEL_WORKSPACE_KEY}"
    custom_log_table_name => "${SENTINEL_WORKSPACE_TABLENAME}"
  }
}
